{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer2vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testProfileDataset(Dataset):\n",
    "    def __init__(self, data_path, meta_path, mask_path, feature_path):\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            import numpy as np\n",
    "            import random\n",
    "        except:\n",
    "            raise(\"pandas or numpy not found\")\n",
    "        try:\n",
    "            self.data = pd.read_pickle(data_path)\n",
    "            self.meta = pd.read_pickle(meta_path)\n",
    "            self.mask = pd.read_pickle(mask_path)\n",
    "            self.feature = pd.read_pickle(feature_path)\n",
    "        except:\n",
    "            raise(\"load data failed\")\n",
    "        self.seed = 19260817\n",
    "        random.seed(self.seed)\n",
    "        only_one_model_batch = []\n",
    "        for k,v in self.feature.items():\n",
    "            if sum(v - np.ones(8)<0) < 2:\n",
    "                only_one_model_batch.append(k)\n",
    "        select_from_meta = []\n",
    "        for model, batch in only_one_model_batch:\n",
    "            s = self.meta[(self.meta.model == model) & (self.meta.batch == batch)]\n",
    "            select_from_meta.append(s)\n",
    "        for x in select_from_meta:\n",
    "            self.meta.drop(x.index, inplace=True)\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gpu, model, batch, length, time_a, gpu_mem_a, cpu_mem_a = self.meta.iloc[idx]\n",
    "        name_path = \"{}_{}_{}\".format(gpu, model, batch)\n",
    "        raw_data = self.data[name_path]\n",
    "        mask_vector = self.mask[name_path]\n",
    "        kernel = raw_data[:, 0]\n",
    "        vector = raw_data[:, 1:13]\n",
    "        vector = vector.astype(np.float32)\n",
    "        feature_vec = self.feature[(model, batch)]\n",
    "        feature_vec = feature_vec.astype(np.float32) / np.max(feature_vec)\n",
    "        return kernel, vector, feature_vec, mask_vector, length, \"{}_{}_{}\".format(gpu, model, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_dataset = testProfileDataset(\"data.pkl\", \"meta.pkl\", \"mask.pkl\", \"feature.pkl\")\n",
    "dataloader = DataLoader(profile_dataset, batch_size=130, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServeRandomMaskModule(nn.Module):\n",
    "    def __init__(self, train_mask_module :RandomMaskModule):\n",
    "        super(ServeRandomMaskModule, self).__init__()\n",
    "        self.embedding_layer = train_mask_module.embedding_layer\n",
    "        self.mlp = train_mask_module.mlp\n",
    "\n",
    "    def forward(self, number_seq_input, vec_seq_input, mask):\n",
    "        # extra_mask = torch.rand(number_seq_input.shape).to(self.device_indicator.device)\n",
    "        # extra_mask = extra_mask < vector_rate\n",
    "        # amsk is extra_mask or mask\n",
    "        # mask = extra_mask | mask\n",
    "\n",
    "        embedded_numbers = self.embedding_layer(number_seq_input)\n",
    "        masked_embedded_numbers = embedded_numbers * (1 - mask.unsqueeze(-1).float())\n",
    "\n",
    "        mlp_output = self.mlp(vec_seq_input)\n",
    "        masked_mlp_output = mlp_output * mask.unsqueeze(-1).float()\n",
    "\n",
    "        output = masked_embedded_numbers + masked_mlp_output\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"correct_model_200.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serve_model = ServeRandomMaskModule(model.mask_module.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.mask_module = serve_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "\n",
    "error_rate_each = []\n",
    "real_rate = []\n",
    "\n",
    "for kernel, vector, feature_vec, mask, length, name in dataloader:\n",
    "    # print(name)\n",
    "    kernel = kernel.to(device)\n",
    "    vector = vector.to(device)\n",
    "    # feature_vec = feature_vec.to(device)\n",
    "    mask = mask.to(device)\n",
    "    length = length.to(device)\n",
    "    o = model(kernel, vector, mask, length)\n",
    "    o = o.cpu()\n",
    "    o = o * (feature_vec > 0)\n",
    "    o_m, _ = torch.max(o, dim=1, keepdim=True)\n",
    "    o = o / o_m\n",
    "    o = o.cpu().detach().numpy()\n",
    "    f = feature_vec.cpu().detach().numpy()\n",
    "    for i in range(len(o)):\n",
    "        error_rate_each.append(np.mean(np.abs(o[i] - f[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error mean:  0.01695101\n",
      "error std:  0.023317022\n",
      "error max:  0.1892632\n"
     ]
    }
   ],
   "source": [
    "error = np.array(error_rate_each)\n",
    "error_non_zero = error[error > 0]\n",
    "print(\"error mean: \", np.mean(error_non_zero))\n",
    "print(\"error std: \", np.std(error_non_zero))\n",
    "print(\"error max: \", np.max(error_non_zero))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ai-scheduler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
